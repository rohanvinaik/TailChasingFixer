"""
LLM Filler Data Detector

Identifies synthetic/placeholder data patterns commonly generated by LLMs,
including lorem ipsum, generic examples, and suspiciously uniform patterns.
"""

import ast
import re
import json
import math
from typing import List, Dict, Any, Optional
from collections import Counter
from pathlib import Path

from .base import Analyzer, AnalysisContext
from ..core.issues import Issue


class LLMFillerDetector(Analyzer):
    """
    Detects LLM-generated filler/placeholder data in code.
    
    Common patterns:
    - Lorem ipsum and variants
    - Generic sequential data (item1, item2, test1, test2)
    - Suspiciously uniform distributions
    - Common LLM example patterns (foo/bar/baz, Alice/Bob)
    - Mock data that's too perfect
    """
    
    name = "llm_filler"
    
    # Known filler text patterns
    LOREM_PATTERNS = [
        r'lorem\s+ipsum',
        r'dolor\s+sit\s+amet',
        r'consectetur\s+adipiscing',
        r'sed\s+do\s+eiusmod',
        r'tempor\s+incididunt',
        r'labore\s+et\s+dolore',
        r'magna\s+aliqua',
        r'ut\s+enim\s+ad\s+minim',
        r'quis\s+nostrud',
        r'exercitation\s+ullamco',
        r'sample\s+text',
        r'placeholder\s+text',
        r'dummy\s+text',
        r'test\s+data',
        r'example\s+data',
    ]
    
    # Generic placeholder names
    PLACEHOLDER_NAMES = {
        # Common programming placeholders
        'foo', 'bar', 'baz', 'qux', 'quux', 'corge', 'grault', 'garply',
        'waldo', 'fred', 'plugh', 'xyzzy', 'thud',
        
        # Generic items
        'item', 'element', 'object', 'thing', 'stuff', 'data',
        'value', 'key', 'entry', 'record', 'row',
        
        # Test names
        'test', 'demo', 'sample', 'example', 'dummy', 'temp', 'tmp',
        
        # Common LLM examples
        'alice', 'bob', 'charlie', 'david', 'eve', 'frank',
        'john', 'jane', 'doe', 'smith',
        
        # Generic products/companies
        'acme', 'widget', 'gadget', 'product', 'company',
        'organization', 'department', 'division',
    }
    
    # Suspicious sequential patterns
    SEQUENTIAL_PATTERNS = [
        (r'item\d+', 'item{n}'),
        (r'test\d+', 'test{n}'),
        (r'example\d+', 'example{n}'),
        (r'data\d+', 'data{n}'),
        (r'value\d+', 'value{n}'),
        (r'key\d+', 'key{n}'),
        (r'field\d+', 'field{n}'),
        (r'column\d+', 'column{n}'),
        (r'row\d+', 'row{n}'),
        (r'user\d+', 'user{n}'),
        (r'[a-z]+_\d+', 'prefix_{n}'),
    ]
    
    # Suspicious uniform patterns in data
    UNIFORM_PATTERNS = {
        'perfect_sequence': lambda vals: all(vals[i] == vals[0] + i for i in range(len(vals))),
        'all_same_length': lambda vals: len(set(len(str(v)) for v in vals)) == 1,
        'round_numbers': lambda vals: all(isinstance(v, (int, float)) and v % 10 == 0 for v in vals),
        'alphabetical': lambda vals: vals == sorted(vals),
        'reverse_alphabetical': lambda vals: vals == sorted(vals, reverse=True),
    }
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        super().__init__(config)
        self.config = config or {}
        
        # Thresholds
        self.min_sequence_length = self.config.get('min_sequence_length', 3)
        self.entropy_threshold = self.config.get('entropy_threshold', 1.5)
        self.repetition_threshold = self.config.get('repetition_threshold', 0.7)
        
    def run(self, ctx: AnalysisContext) -> List[Issue]:
        """Analyze code for LLM filler data patterns."""
        issues = []
        
        for file_path, tree in ctx.ast_index.items():
            # Skip test files (they legitimately use test data)
            if self._is_test_file(file_path):
                continue
                
            file_issues = self._analyze_file(file_path, tree, ctx)
            issues.extend(file_issues)
        
        return issues
    
    def _analyze_file(self, file_path: str, tree: ast.AST, ctx: AnalysisContext) -> List[Issue]:
        """Analyze a single file for filler data."""
        issues = []
        
        # Check string literals
        for node in ast.walk(tree):
            if isinstance(node, ast.Constant) and isinstance(node.value, str):
                issue = self._check_string_literal(node.value, file_path, node.lineno)
                if issue:
                    issues.append(issue)
            
            # Check list/tuple literals
            elif isinstance(node, (ast.List, ast.Tuple)):
                issue = self._check_sequence_literal(node, file_path)
                if issue:
                    issues.append(issue)
            
            # Check dictionary literals
            elif isinstance(node, ast.Dict):
                issue = self._check_dict_literal(node, file_path)
                if issue:
                    issues.append(issue)
            
            # Check variable assignments with suspicious patterns
            elif isinstance(node, ast.Assign):
                issue = self._check_assignment(node, file_path)
                if issue:
                    issues.append(issue)
        
        # Check for suspicious data in docstrings
        docstring_issues = self._check_docstrings(tree, file_path)
        issues.extend(docstring_issues)
        
        # Check JSON data in strings
        json_issues = self._check_json_data(tree, file_path)
        issues.extend(json_issues)
        
        return issues
    
    def _check_string_literal(self, text: str, file_path: str, line: int) -> Optional[Issue]:
        """Check if a string contains filler text."""
        if len(text) < 20:  # Skip short strings
            return None
        
        text_lower = text.lower()
        
        # Check for lorem ipsum
        for pattern in self.LOREM_PATTERNS:
            if re.search(pattern, text_lower):
                return Issue(
                    kind="llm_filler_text",
                    file=file_path,
                    line=line,
                    message=f"Lorem ipsum or placeholder text detected",
                    severity=2,
                    evidence={
                        'text_snippet': text[:100],
                        'pattern': pattern,
                        'filler_type': 'lorem_ipsum'
                    }
                )
        
        # Check for excessive repetition
        words = text_lower.split()
        if len(words) > 5:
            word_counts = Counter(words)
            most_common = word_counts.most_common(1)[0]
            repetition_ratio = most_common[1] / len(words)
            
            if repetition_ratio > self.repetition_threshold:
                return Issue(
                    kind="llm_filler_text",
                    file=file_path,
                    line=line,
                    message=f"Suspiciously repetitive text pattern",
                    severity=2,
                    evidence={
                        'text_snippet': text[:100],
                        'repetition_ratio': repetition_ratio,
                        'most_repeated': most_common[0],
                        'filler_type': 'repetitive'
                    }
                )
        
        # Check entropy (too uniform = likely synthetic)
        if len(text) > 50:
            entropy = self._calculate_entropy(text)
            if entropy < self.entropy_threshold:
                return Issue(
                    kind="llm_filler_text",
                    file=file_path,
                    line=line,
                    message=f"Suspiciously low entropy in text (likely synthetic)",
                    severity=2,
                    evidence={
                        'text_snippet': text[:100],
                        'entropy': entropy,
                        'filler_type': 'low_entropy'
                    }
                )
        
        return None
    
    def _check_sequence_literal(self, node: ast.AST, file_path: str) -> Optional[Issue]:
        """Check list/tuple for filler patterns."""
        elements = node.elts
        
        if len(elements) < self.min_sequence_length:
            return None
        
        # Extract string values
        str_values = []
        for elem in elements:
            if isinstance(elem, ast.Constant) and isinstance(elem.value, str):
                str_values.append(elem.value)
        
        if not str_values:
            return None
        
        # Check for sequential patterns (item1, item2, item3)
        for pattern, template in self.SEQUENTIAL_PATTERNS:
            matches = [re.match(pattern, v) for v in str_values]
            if all(matches):
                # Check if they follow a perfect sequence
                numbers = []
                for match in matches:
                    if match:
                        # Extract number from the match
                        num_str = re.search(r'\d+', match.group())
                        if num_str:
                            numbers.append(int(num_str.group()))
                
                if numbers and self._is_perfect_sequence(numbers):
                    return Issue(
                        kind="llm_filler_sequence",
                        file=file_path,
                        line=node.lineno,
                        message=f"Sequential placeholder pattern detected: {template}",
                        severity=3,
                        evidence={
                            'pattern': template,
                            'values': str_values[:10],  # First 10 for evidence
                            'sequence_length': len(str_values),
                            'filler_type': 'sequential'
                        }
                    )
        
        # Check for all placeholder names
        if all(v.lower() in self.PLACEHOLDER_NAMES for v in str_values):
            return Issue(
                kind="llm_filler_sequence",
                file=file_path,
                line=node.lineno,
                message=f"List contains only generic placeholder names",
                severity=3,
                evidence={
                    'values': str_values[:10],
                    'filler_type': 'placeholder_names'
                }
            )
        
        # Check for suspicious uniformity
        for pattern_name, check_func in self.UNIFORM_PATTERNS.items():
            try:
                if check_func(str_values):
                    return Issue(
                        kind="llm_filler_sequence",
                        file=file_path,
                        line=node.lineno,
                        message=f"Suspiciously uniform pattern: {pattern_name}",
                        severity=2,
                        evidence={
                            'pattern': pattern_name,
                            'values': str_values[:10],
                            'filler_type': 'uniform'
                        }
                    )
            except:
                pass  # Skip if check fails
        
        return None
    
    def _check_dict_literal(self, node: ast.Dict, file_path: str) -> Optional[Issue]:
        """Check dictionary for filler patterns."""
        if not node.keys:
            return None
        
        # Extract string keys
        str_keys = []
        for key in node.keys:
            if isinstance(key, ast.Constant) and isinstance(key.value, str):
                str_keys.append(key.value)
        
        if len(str_keys) < self.min_sequence_length:
            return None
        
        # Check for sequential key patterns
        for pattern, template in self.SEQUENTIAL_PATTERNS:
            if all(re.match(pattern, k) for k in str_keys):
                return Issue(
                    kind="llm_filler_dict",
                    file=file_path,
                    line=node.lineno,
                    message=f"Dictionary with sequential placeholder keys: {template}",
                    severity=3,
                    evidence={
                        'pattern': template,
                        'keys': str_keys[:10],
                        'filler_type': 'sequential_keys'
                    }
                )
        
        # Check if all keys are placeholders
        if all(k.lower() in self.PLACEHOLDER_NAMES for k in str_keys):
            return Issue(
                kind="llm_filler_dict",
                file=file_path,
                line=node.lineno,
                message=f"Dictionary contains only placeholder keys",
                severity=3,
                evidence={
                    'keys': str_keys[:10],
                    'filler_type': 'placeholder_keys'
                }
            )
        
        return None
    
    def _check_assignment(self, node: ast.Assign, file_path: str) -> Optional[Issue]:
        """Check variable assignments for filler patterns."""
        # Look for assignments like: data = ["test1", "test2", "test3"]
        if isinstance(node.value, (ast.List, ast.Tuple)):
            return self._check_sequence_literal(node.value, file_path)
        elif isinstance(node.value, ast.Dict):
            return self._check_dict_literal(node.value, file_path)
        
        return None
    
    def _check_docstrings(self, tree: ast.AST, file_path: str) -> List[Issue]:
        """Check docstrings for filler content."""
        issues = []
        
        for node in ast.walk(tree):
            if isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.Module)):
                docstring = ast.get_docstring(node)
                if docstring:
                    # Check for TODO/FIXME with generic descriptions
                    if re.search(r'TODO:\s*(add|implement|write|fill)\s+(description|documentation|details?|implementation)', docstring, re.I):
                        issues.append(Issue(
                            kind="llm_filler_docstring",
                            file=file_path,
                            line=node.lineno if hasattr(node, 'lineno') else 1,
                            message="Generic TODO in docstring",
                            severity=2,
                            evidence={
                                'docstring_snippet': docstring[:200],
                                'filler_type': 'generic_todo'
                            }
                        ))
                    
                    # Check for example code with foo/bar
                    if 'Example:' in docstring or '>>>' in docstring:
                        example_uses_placeholder = any(
                            ph in docstring.lower() 
                            for ph in ['foo', 'bar', 'baz', 'lorem', 'ipsum']
                        )
                        if example_uses_placeholder:
                            issues.append(Issue(
                                kind="llm_filler_docstring",
                                file=file_path,
                                line=node.lineno if hasattr(node, 'lineno') else 1,
                                message="Documentation example uses generic placeholders",
                                severity=2,
                                evidence={
                                    'docstring_snippet': docstring[:200],
                                    'filler_type': 'placeholder_example'
                                }
                            ))
        
        return issues
    
    def _check_json_data(self, tree: ast.AST, file_path: str) -> List[Issue]:
        """Check for JSON data embedded in strings."""
        issues = []
        
        for node in ast.walk(tree):
            if isinstance(node, ast.Constant) and isinstance(node.value, str):
                # Try to parse as JSON
                try:
                    data = json.loads(node.value)
                    if isinstance(data, dict):
                        # Check for placeholder patterns in JSON
                        if self._is_filler_json(data):
                            issues.append(Issue(
                                kind="llm_filler_json",
                                file=file_path,
                                line=node.lineno,
                                message="JSON data contains placeholder/filler content",
                                severity=3,
                                evidence={
                                    'json_keys': list(data.keys())[:10],
                                    'filler_type': 'json_placeholders'
                                }
                            ))
                except:
                    pass  # Not JSON
        
        return issues
    
    def _is_filler_json(self, data: dict) -> bool:
        """Check if JSON data contains filler patterns."""
        # Check keys
        keys = list(data.keys())
        if all(k.lower() in self.PLACEHOLDER_NAMES for k in keys):
            return True
        
        # Check for sequential keys
        for pattern, _ in self.SEQUENTIAL_PATTERNS:
            if all(re.match(pattern, k) for k in keys):
                return True
        
        # Check values recursively
        for value in data.values():
            if isinstance(value, str):
                if value.lower() in self.PLACEHOLDER_NAMES:
                    return True
                for pattern in self.LOREM_PATTERNS:
                    if re.search(pattern, value.lower()):
                        return True
            elif isinstance(value, list) and len(value) > 2:
                if all(isinstance(v, str) and v.lower() in self.PLACEHOLDER_NAMES for v in value):
                    return True
        
        return False
    
    def _calculate_entropy(self, text: str) -> float:
        """Calculate Shannon entropy of text."""
        if not text:
            return 0.0
        
        # Character frequency
        freq = Counter(text)
        probs = [count / len(text) for count in freq.values()]
        
        # Shannon entropy
        entropy = -sum(p * math.log2(p) for p in probs if p > 0)
        return entropy
    
    def _is_perfect_sequence(self, numbers: List[int]) -> bool:
        """Check if numbers form a perfect sequence."""
        if len(numbers) < 2:
            return False
        
        # Check for arithmetic sequence
        diffs = [numbers[i+1] - numbers[i] for i in range(len(numbers)-1)]
        return len(set(diffs)) == 1  # All differences are the same
    
    def _is_test_file(self, file_path: str) -> bool:
        """Check if this is a test file."""
        path = Path(file_path)
        return (
            'test' in path.name.lower() or
            'test' in str(path.parent).lower() or
            path.name.startswith('test_') or
            path.name.endswith('_test.py')
        )